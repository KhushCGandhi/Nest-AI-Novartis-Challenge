{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10340682,"sourceType":"datasetVersion","datasetId":6309932}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T05:53:42.005038Z","iopub.execute_input":"2025-02-27T05:53:42.005281Z","iopub.status.idle":"2025-02-27T05:53:42.974609Z","shell.execute_reply.started":"2025-02-27T05:53:42.005242Z","shell.execute_reply":"2025-02-27T05:53:42.973668Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nest-competition/676e54b2807db_usecase_2_test_gt_removed.csv\n/kaggle/input/nest-competition/X_train_text2.npy\n/kaggle/input/nest-competition/eligibilities.txt\n/kaggle/input/nest-competition/facilities_cleaned.csv\n/kaggle/input/nest-competition/text_features.npy\n/kaggle/input/nest-competition/y_train.npy\n/kaggle/input/nest-competition/eligibilities_cleaned.csv\n/kaggle/input/nest-competition/y_train.pkl\n/kaggle/input/nest-competition/y_test.npy\n/kaggle/input/nest-competition/drop_withdrawals_cleaned.csv\n/kaggle/input/nest-competition/X_train_combined (1).pkl\n/kaggle/input/nest-competition/X_test_text.npy\n/kaggle/input/nest-competition/X_test_num.npy\n/kaggle/input/nest-competition/X_train_num.npy\n/kaggle/input/nest-competition/facilities.txt\n/kaggle/input/nest-competition/tfidf.npy\n/kaggle/input/nest-competition/usecase_2_.csv\n/kaggle/input/nest-competition/X_test_combined (1).pkl\n/kaggle/input/nest-competition/y_test.pkl\n/kaggle/input/nest-competition/drop_withdrawals.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom scipy.sparse import csr_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import hstack\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T05:53:42.975411Z","iopub.execute_input":"2025-02-27T05:53:42.975761Z","iopub.status.idle":"2025-02-27T05:53:51.836885Z","shell.execute_reply.started":"2025-02-27T05:53:42.975729Z","shell.execute_reply":"2025-02-27T05:53:51.836240Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nest-competition/usecase_2_.csv')\n# /kaggle/input/nest-competition/usecase_2_.csv\ndf = df.rename(columns={\"NCT Number\": \"nct_id\"})\nfacilities_cleaned = pd.read_csv('/kaggle/input/nest-competition/facilities_cleaned.csv')\neligibilities_cleaned = pd.read_csv('/kaggle/input/nest-competition/eligibilities_cleaned.csv')\ndrop_withdrawals_cleaned = pd.read_csv('/kaggle/input/nest-competition/drop_withdrawals_cleaned.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T05:53:51.837713Z","iopub.execute_input":"2025-02-27T05:53:51.838170Z","iopub.status.idle":"2025-02-27T05:53:59.504377Z","shell.execute_reply.started":"2025-02-27T05:53:51.838133Z","shell.execute_reply":"2025-02-27T05:53:59.503589Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Preprocessing usecase_2 (main file)","metadata":{}},{"cell_type":"code","source":"df.drop_duplicates(keep=\"first\", inplace=True)\n\nmerged_df = df.merge(facilities_cleaned, on='nct_id', how='left', suffixes=('', '_facilities'))\nmerged_df = merged_df.merge(eligibilities_cleaned, on='nct_id', how='left', suffixes=('', '_eligibilities'))\nmerged_df = merged_df.merge(drop_withdrawals_cleaned, on='nct_id', how='left', suffixes=('', '_withdrawals'))\n\ncolumns_to_fill = ['total_dropouts', 'unique_reasons', 'unique_periods']\nmerged_df[columns_to_fill] = merged_df[columns_to_fill].fillna(0)\n\ncategories = ['ADULT', 'OLDER_ADULT', 'CHILD']\n\nfor category in categories:\n    merged_df[category] = merged_df['Age'].str.contains(category, case=False, na=False).astype(int)\n\nmerged_df = merged_df.drop(['adult', 'child', 'older_adult', 'Age'], axis=1)\n\nmerged_df['gender_based'] = merged_df['gender_based'].fillna('f')\nmerged_df['Sex'] = merged_df['Sex'].fillna('ALL')\nmerged_df = merged_df.drop('gender', axis=1)\n\nphases = ['EARLY_PHASE1', 'PHASE1', 'PHASE2', 'PHASE3', 'PHASE4']\n\nfor phase in phases:\n    merged_df[phase] = merged_df['Phases'].str.contains(phase, case=False, na=False).astype(int)\n\nmerged_df['Enrollment'] = merged_df['Enrollment'].fillna(merged_df['Enrollment'].median())\nmerged_df = merged_df.drop('Locations', axis=1)\n\nfunder_type_encoded = pd.get_dummies(merged_df['Funder Type'], prefix='Funder_Type', drop_first=False)\nmerged_df = pd.concat([merged_df, funder_type_encoded], axis=1)\nmerged_df = merged_df.drop('Funder Type', axis=1)\n\nmerged_df['minimum_age'] = merged_df['minimum_age'].fillna('0 Minutes')\nmerged_df['maximum_age'] = merged_df['maximum_age'].fillna('200 Years')\n\nmerged_df['healthy_volunteers'] = merged_df['healthy_volunteers'].fillna('f')\n\ndef split_criteria(row):\n    if pd.isna(row):\n        return pd.Series([pd.NA, pd.NA])\n\n    inclusion = exclusion = \"\"\n\n    if \"inclusion\" in row.lower():\n        inclusion_part = row.lower().split(\"inclusion\", 1)[1]\n        if \"exclusion\" in inclusion_part:\n            inclusion = inclusion_part.split(\"exclusion\", 1)[0].strip()\n        else:\n            inclusion = inclusion_part.strip()\n    \n    if \"exclusion\" in row.lower():\n        exclusion_part = row.lower().split(\"exclusion\", 1)[1]\n        exclusion = exclusion_part.strip()\n\n    inclusion = inclusion.replace(\"criteria:\", \"\").strip()\n    exclusion = exclusion.replace(\"criteria:\", \"\").strip()\n\n    return pd.Series([inclusion, exclusion])\n\nmerged_df[['inclusion_criteria', 'exclusion_criteria']] = merged_df['criteria'].apply(split_criteria)\nmerged_df = merged_df.drop(columns=['criteria'])\n\nmerged_df['unique_name_count'] = merged_df['unique_name_count'].fillna(merged_df['unique_name_count'].median())\nmerged_df['unique_city_count'] = merged_df['unique_city_count'].fillna(merged_df['unique_city_count'].median())\nmerged_df['unique_zip_count'] = merged_df['unique_zip_count'].fillna(merged_df['unique_zip_count'].median())\nmerged_df['unique_state_count'] = merged_df['unique_state_count'].fillna(merged_df['unique_state_count'].median())\nmerged_df['unique_country_count'] = merged_df['unique_country_count'].fillna(merged_df['unique_country_count'].median())\n\ndef bin_and_encode_all(df, columns, high_cutoff=0.7, medium_cutoff=0.9):\n    def calculate_thresholds(value_counts, high_cutoff, medium_cutoff):\n        total = value_counts.sum()\n        cumulative_percentage = value_counts.cumsum() / total\n        high_threshold = value_counts[cumulative_percentage <= high_cutoff].iloc[-1]\n        medium_threshold = value_counts[cumulative_percentage <= medium_cutoff].iloc[-1]\n        return high_threshold, medium_threshold\n\n    for column in columns:\n        counts = df[column].value_counts(dropna=False)  # Include NaN in counts\n        high_threshold, medium_threshold = calculate_thresholds(counts, high_cutoff, medium_cutoff)\n\n        def assign_bins(value):\n            if pd.isna(value):\n                return \"Missing\"  # Handle NaN explicitly\n            count = counts.get(value, 0)\n            if count > high_threshold:\n                return \"High_Freq\"\n            elif count >= medium_threshold:\n                return \"Medium_Freq\"\n            else:\n                return \"Low_Freq\"\n\n        df[f\"{column}_bin\"] = df[column].apply(assign_bins)\n\n        one_hot = pd.get_dummies(df[f\"{column}_bin\"], prefix=f\"{column}_Bin\").astype(int)\n\n        df = pd.concat([df, one_hot], axis=1).drop(f\"{column}_bin\", axis=1)\n\n    df = df.drop(columns, axis=1)\n\n    return df\n\ncolumns_to_process = ['mode_country', 'mode_city', 'mode_state', 'mode_name']\nmerged_df = bin_and_encode_all(merged_df, columns_to_process)\n\nboolean_columns = [col for col in merged_df.columns if merged_df[col].dtype == 'bool']\n\nmerged_df['Study Results'] = merged_df['Study Results'].map({'YES': 1, 'NO': 0})\n\nmerged_df['healthy_volunteers'] = merged_df['healthy_volunteers'].map({'t': 1, 'f': 0})\n\nmerged_df['gender_based'] = merged_df['gender_based'].map({'t': 1, 'f': 0})\n\nsex_encoded = pd.get_dummies(merged_df['Sex'], prefix='Sex')\nmerged_df = pd.concat([merged_df, sex_encoded], axis=1)\n\nfor col in ['Sex_ALL', 'Sex_FEMALE', 'Sex_MALE']:\n    if col in merged_df.columns:\n        merged_df[col] = merged_df[col].astype(int)\n\nfor col in boolean_columns:\n    merged_df[col] = merged_df[col].astype(int)\n\nmerged_df = merged_df.drop('Sex', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T05:53:59.505906Z","iopub.execute_input":"2025-02-27T05:53:59.506228Z","iopub.status.idle":"2025-02-27T05:54:11.159834Z","shell.execute_reply.started":"2025-02-27T05:53:59.506206Z","shell.execute_reply":"2025-02-27T05:54:11.158912Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"data = merged_df.copy()\ndel merged_df\n\nbin_features = [\"Unnamed: 0\", \"Study URL\", \"Acronym\", \"Study Status\", \"Other IDs\", \"Study Documents\"]\ntext_columns = ['Study Title', 'Brief Summary', 'Conditions',\n       'Interventions', 'Primary Outcome Measures', 'exclusion_criteria',\n       'Secondary Outcome Measures', 'Other Outcome Measures', 'Study Design', 'inclusion_criteria']\n\ndata = data.drop(bin_features, axis = 1)\n\ndata['Secondary Outcome Measures'].fillna('', inplace=True)\ndata['Other Outcome Measures'].fillna('', inplace=True)\ndata['inclusion_criteria'].fillna(\"\", inplace=True)\ndata['exclusion_criteria'].fillna(\"\", inplace=True)\n\npd.set_option('display.max_columns', None)\n\nconversion_factors = {\n    'Minute': 1,\n    'Hour': 60,\n    'Day': 1440,          # 24 hours × 60 minutes\n    'Week': 10080,        # 7 days × 1440 minutes\n    'Month': 43800,       # Assuming 30.44 days per month on average\n    'Year': 525600        # 365.25 days × 1440 minutes\n}\n\nimport re\n\ndef convert_to_minutes(time_str):\n    match = re.match(r\"(\\d+)\\s*(\\w+)\", time_str)  # Extract number and unit\n    if match:\n        value, unit = int(match.group(1)), match.group(2).capitalize().rstrip('s')\n        return value * conversion_factors.get(unit, 1)  # Default to 1 if unit not found\n    return 0\n\ndata['minimum_age_min'] = data['minimum_age'].apply(convert_to_minutes)\ndata['maximum_age_min'] = data['maximum_age'].apply(convert_to_minutes)\n\ndata['minimum_age_min'] = np.log1p(data['minimum_age_min'])\ndata['maximum_age_min'] = np.log1p(data['minimum_age_min'])\n\nnumerical_columns = [\"Study Results\", \"Enrollment\", 'unique_name_count',\n       'unique_state_count', 'unique_zip_count', 'unique_city_count',\n       'unique_country_count', 'minimum_age_min', 'maximum_age_min',\n        'healthy_volunteers', 'gender_based', 'total_dropouts',\n       'unique_reasons', 'unique_periods', 'ADULT', 'OLDER_ADULT', 'CHILD', 'EARLY_PHASE1', 'PHASE1', 'PHASE2',\n       'PHASE3', 'PHASE4', 'Funder_Type_FED', 'Funder_Type_INDIV',\n       'Funder_Type_INDUSTRY', 'Funder_Type_NETWORK', 'Funder_Type_NIH',\n       'Funder_Type_OTHER', 'Funder_Type_OTHER_GOV', 'Funder_Type_UNKNOWN',\n        'mode_country_Bin_High_Freq', 'mode_country_Bin_Low_Freq',\n       'mode_country_Bin_Medium_Freq', 'mode_country_Bin_Missing',\n       'mode_city_Bin_High_Freq', 'mode_city_Bin_Low_Freq',\n       'mode_city_Bin_Medium_Freq', 'mode_city_Bin_Missing',\n       'mode_state_Bin_High_Freq', 'mode_state_Bin_Low_Freq',\n       'mode_state_Bin_Medium_Freq', 'mode_state_Bin_Missing',\n       'mode_name_Bin_High_Freq', 'mode_name_Bin_Medium_Freq',\n       'mode_name_Bin_Missing', 'Sex_ALL', 'Sex_FEMALE', 'Sex_MALE' ]\nscaler = StandardScaler()\ndata[numerical_columns] = scaler.fit_transform(data[numerical_columns])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T05:54:11.161134Z","iopub.execute_input":"2025-02-27T05:54:11.161464Z","iopub.status.idle":"2025-02-27T05:54:11.704743Z","shell.execute_reply.started":"2025-02-27T05:54:11.161443Z","shell.execute_reply":"2025-02-27T05:54:11.704010Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-5-93a9baa31531>:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['Secondary Outcome Measures'].fillna('', inplace=True)\n<ipython-input-5-93a9baa31531>:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['Other Outcome Measures'].fillna('', inplace=True)\n<ipython-input-5-93a9baa31531>:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['inclusion_criteria'].fillna(\"\", inplace=True)\n<ipython-input-5-93a9baa31531>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['exclusion_criteria'].fillna(\"\", inplace=True)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Textual Feature Engineering","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T05:54:11.705758Z","iopub.execute_input":"2025-02-27T05:54:11.706046Z","iopub.status.idle":"2025-02-27T05:54:13.025887Z","shell.execute_reply.started":"2025-02-27T05:54:11.706013Z","shell.execute_reply":"2025-02-27T05:54:13.024979Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport string\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_text_for_tfidf(text):\n    # Rigorous preprocessing for TF-IDF\n    if isinstance(text, str):\n        text = text.lower()\n        text = ''.join(ch for ch in text if ch not in string.punctuation)  # Remove punctuation\n        tokens = text.split()\n        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Lemmatize and remove stopwords\n        return ' '.join(tokens)\n    return ''\n\ntfidf_columns = ['Conditions', 'Interventions', 'Secondary Outcome Measures', 'Other Outcome Measures', 'inclusion_criteria']\nfor col in tfidf_columns:\n    data[col] = data[col].apply(preprocess_text_for_tfidf)\n\n# Define TF-IDF vectorizer with limited max features\ntfidf_vectorizers = {\n    col: TfidfVectorizer(max_features=500, stop_words='english')\n    for col in ['Conditions', 'Interventions', 'Secondary Outcome Measures', 'Other Outcome Measures', 'inclusion_criteria']\n}\n\n# Fit and transform TF-IDF features\ntfidf_features = {}\nfor col, vectorizer in tfidf_vectorizers.items():\n    tfidf_features[col] = vectorizer.fit_transform(data[col])\n\ngc.collect()\n# Combine TF-IDF vectors into a single sparse matrix\ntfidf_combined = hstack([tfidf_features[col] for col in tfidf_features])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T05:54:13.026942Z","iopub.execute_input":"2025-02-27T05:54:13.027276Z","iopub.status.idle":"2025-02-27T05:55:29.900954Z","shell.execute_reply.started":"2025-02-27T05:54:13.027251Z","shell.execute_reply":"2025-02-27T05:55:29.900003Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"del df, facilities_cleaned, eligibilities_cleaned, drop_withdrawals_cleaned\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T05:55:29.903828Z","iopub.execute_input":"2025-02-27T05:55:29.904113Z","iopub.status.idle":"2025-02-27T05:55:30.286780Z","shell.execute_reply.started":"2025-02-27T05:55:29.904085Z","shell.execute_reply":"2025-02-27T05:55:30.286000Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T06:08:47.958413Z","iopub.execute_input":"2025-02-27T06:08:47.958717Z","iopub.status.idle":"2025-02-27T06:08:47.963951Z","shell.execute_reply.started":"2025-02-27T06:08:47.958691Z","shell.execute_reply":"2025-02-27T06:08:47.963037Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(68960, 70)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}