{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10323044,"sourceType":"datasetVersion","datasetId":6309932}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom scipy.sparse import csr_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import hstack\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:26:18.646432Z","iopub.execute_input":"2024-12-30T06:26:18.646745Z","iopub.status.idle":"2024-12-30T06:26:23.168578Z","shell.execute_reply.started":"2024-12-30T06:26:18.646721Z","shell.execute_reply":"2024-12-30T06:26:23.167835Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nest-competition/usecase_2_.csv')\ndf = df.rename(columns={\"NCT Number\": \"nct_id\"})\nfacilities_cleaned = pd.read_csv('/kaggle/input/nest-competition/facilities_cleaned.csv')\neligibilities_cleaned = pd.read_csv('/kaggle/input/nest-competition/eligibilities_cleaned.csv')\ndrop_withdrawals_cleaned = pd.read_csv('/kaggle/input/nest-competition/drop_withdrawals_cleaned.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:26:23.169538Z","iopub.execute_input":"2024-12-30T06:26:23.169880Z","iopub.status.idle":"2024-12-30T06:26:30.402585Z","shell.execute_reply.started":"2024-12-30T06:26:23.169860Z","shell.execute_reply":"2024-12-30T06:26:30.401824Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"X_train_num = X_train_num.astype('float32')\nX_test_num = X_test_num.astype('float32')\nX_train_text = X_train_text.astype('float32')\nX_test_text = X_test_text.astype('float32')\n\ndel text_features, all_text_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:28:33.040575Z","iopub.execute_input":"2024-12-30T06:28:33.040863Z","iopub.status.idle":"2024-12-30T06:28:41.239924Z","shell.execute_reply.started":"2024-12-30T06:28:33.040834Z","shell.execute_reply":"2024-12-30T06:28:41.239019Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:28:41.241851Z","iopub.execute_input":"2024-12-30T06:28:41.242088Z","iopub.status.idle":"2024-12-30T06:28:41.441762Z","shell.execute_reply.started":"2024-12-30T06:28:41.242069Z","shell.execute_reply":"2024-12-30T06:28:41.440854Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:28:41.442951Z","iopub.execute_input":"2024-12-30T06:28:41.443201Z","iopub.status.idle":"2024-12-30T06:28:41.456289Z","shell.execute_reply.started":"2024-12-30T06:28:41.443180Z","shell.execute_reply":"2024-12-30T06:28:41.455406Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class TextEncoder(nn.Module):\n    def __init__(self, input_dim, latent_dim, dropout=0.2):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 1024),\n            nn.LayerNorm(1024),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, 512),\n            nn.LayerNorm(512),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(512, latent_dim)\n        )\n        \n    def forward(self, x):\n        return self.encoder(x)\n\nclass TextDecoder(nn.Module):\n    def __init__(self, latent_dim, output_dim, dropout=0.2):\n        super().__init__()\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 512),\n            nn.LayerNorm(512),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(512, 1024),\n            nn.LayerNorm(1024),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(1024, output_dim)\n        )\n        \n    def forward(self, x):\n        return self.decoder(x)\n\nclass EnrollmentPredictor(nn.Module):\n    def __init__(self, latent_dim, num_features, dropout=0.2):\n        super().__init__()\n        combined_dim = latent_dim + num_features\n        self.predictor = nn.Sequential(\n            nn.Linear(combined_dim, 256),\n            nn.LayerNorm(256),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, 128),\n            nn.LayerNorm(128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1)\n        )\n        \n    def forward(self, latent, num_features):\n        combined = torch.cat([latent, num_features], dim=1)\n        return self.predictor(combined)\n\nclass EnrollmentModel(nn.Module):\n    def __init__(self, text_dim, num_features, latent_dim=128):\n        super().__init__()\n        self.text_encoder = TextEncoder(text_dim, latent_dim)\n        self.text_decoder = TextDecoder(latent_dim, text_dim)\n        self.predictor = EnrollmentPredictor(latent_dim, num_features)\n        \n    def forward(self, text_features, num_features):\n        latent = self.text_encoder(text_features)\n        reconstructed = self.text_decoder(latent)\n        prediction = self.predictor(latent, num_features)\n        return prediction, reconstructed, latent\n\n# Training utilities\nclass EnrollmentDataset(Dataset):\n    def __init__(self, text_features, num_features, targets):\n        self.text_features = torch.FloatTensor(text_features.toarray())\n        self.num_features = torch.FloatTensor(num_features)\n        self.targets = torch.FloatTensor(targets)\n        \n    def __len__(self):\n        return len(self.targets)\n    \n    def __getitem__(self, idx):\n        return (self.text_features[idx], self.num_features[idx], self.targets[idx])\n\ndef train_epoch(model, train_loader, optimizer, device, alpha=0.3):\n    model.train()\n    total_loss = 0\n    for text_batch, num_batch, target_batch in train_loader:\n        text_batch = text_batch.to(device)\n        num_batch = num_batch.to(device)\n        target_batch = target_batch.to(device).unsqueeze(1)\n        \n        optimizer.zero_grad()\n        pred, reconstructed, _ = model(text_batch, num_batch)\n        \n        # Combine reconstruction loss and prediction loss\n        recon_loss = F.mse_loss(reconstructed, text_batch)\n        pred_loss = F.mse_loss(pred, target_batch)\n        loss = alpha * recon_loss + (1 - alpha) * pred_loss\n        \n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    return total_loss / len(train_loader)\n\ndef validate(model, val_loader, device):\n    model.eval()\n    predictions = []\n    targets = []\n    \n    with torch.no_grad():\n        for text_batch, num_batch, target_batch in val_loader:\n            text_batch = text_batch.to(device)\n            num_batch = num_batch.to(device)\n            pred, _, _ = model(text_batch, num_batch)\n            predictions.extend(pred.cpu().numpy())\n            targets.extend(target_batch.numpy())\n    \n    predictions = np.array(predictions)\n    targets = np.array(targets)\n    \n    rmse = np.sqrt(mean_squared_error(targets, predictions))\n    r2 = r2_score(targets, predictions)\n    \n    return rmse, r2\n\n# Training setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbatch_size = 64\nnum_epochs = 30\nlatent_dim = 256\n\n# Create datasets and dataloaders\ntrain_dataset = EnrollmentDataset(X_train_text, X_train_num, y_train)\ntest_dataset = EnrollmentDataset(X_test_text, X_test_num, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# Initialize model\nmodel = EnrollmentModel(X_train_text.shape[1], X_train_num.shape[1], latent_dim).to(device)\noptimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:28:41.457261Z","iopub.execute_input":"2024-12-30T06:28:41.457632Z","iopub.status.idle":"2024-12-30T06:28:47.000999Z","shell.execute_reply.started":"2024-12-30T06:28:41.457601Z","shell.execute_reply":"2024-12-30T06:28:47.000206Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Training loop\nbest_rmse = float('inf')\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(model, train_loader, optimizer, device)\n    val_rmse, val_r2 = validate(model, test_loader, device)\n    \n    scheduler.step(val_rmse)\n    \n    if val_rmse < best_rmse:\n        best_rmse = val_rmse\n        torch.save(model.state_dict(), 'best_model.pt')\n    \n    print(f'Epoch {epoch+1}/{num_epochs}:')\n    print(f'Train Loss: {train_loss:.4f}')\n    print(f'Val RMSE: {val_rmse:.4f}, Val R2: {val_r2:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:28:47.001784Z","iopub.execute_input":"2024-12-30T06:28:47.002145Z","iopub.status.idle":"2024-12-30T06:32:45.651083Z","shell.execute_reply.started":"2024-12-30T06:28:47.002124Z","shell.execute_reply":"2024-12-30T06:32:45.650153Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30:\nTrain Loss: 174.6086\nVal RMSE: 14.2235, Val R2: 0.2452\nEpoch 2/30:\nTrain Loss: 122.2501\nVal RMSE: 13.1322, Val R2: 0.3566\nEpoch 3/30:\nTrain Loss: 115.2578\nVal RMSE: 12.9615, Val R2: 0.3732\nEpoch 4/30:\nTrain Loss: 111.8931\nVal RMSE: 12.8940, Val R2: 0.3797\nEpoch 5/30:\nTrain Loss: 107.7375\nVal RMSE: 12.7003, Val R2: 0.3982\nEpoch 6/30:\nTrain Loss: 105.5436\nVal RMSE: 12.6941, Val R2: 0.3988\nEpoch 7/30:\nTrain Loss: 102.1707\nVal RMSE: 12.9951, Val R2: 0.3699\nEpoch 8/30:\nTrain Loss: 99.4927\nVal RMSE: 12.6687, Val R2: 0.4012\nEpoch 9/30:\nTrain Loss: 96.5164\nVal RMSE: 12.9885, Val R2: 0.3706\nEpoch 10/30:\nTrain Loss: 92.8839\nVal RMSE: 12.7272, Val R2: 0.3957\nEpoch 11/30:\nTrain Loss: 90.8528\nVal RMSE: 12.6369, Val R2: 0.4042\nEpoch 12/30:\nTrain Loss: 87.0073\nVal RMSE: 12.6129, Val R2: 0.4065\nEpoch 13/30:\nTrain Loss: 83.4892\nVal RMSE: 12.9036, Val R2: 0.3788\nEpoch 14/30:\nTrain Loss: 79.8507\nVal RMSE: 12.7793, Val R2: 0.3907\nEpoch 15/30:\nTrain Loss: 77.0928\nVal RMSE: 13.3956, Val R2: 0.3305\nEpoch 16/30:\nTrain Loss: 74.2848\nVal RMSE: 12.9296, Val R2: 0.3763\nEpoch 17/30:\nTrain Loss: 70.1420\nVal RMSE: 13.0383, Val R2: 0.3657\nEpoch 18/30:\nTrain Loss: 67.8173\nVal RMSE: 13.3368, Val R2: 0.3364\nEpoch 19/30:\nTrain Loss: 56.0340\nVal RMSE: 13.1397, Val R2: 0.3558\nEpoch 20/30:\nTrain Loss: 52.4932\nVal RMSE: 12.9702, Val R2: 0.3724\nEpoch 21/30:\nTrain Loss: 49.5556\nVal RMSE: 13.2109, Val R2: 0.3488\nEpoch 22/30:\nTrain Loss: 45.9404\nVal RMSE: 13.0805, Val R2: 0.3616\nEpoch 23/30:\nTrain Loss: 43.5678\nVal RMSE: 13.1415, Val R2: 0.3557\nEpoch 24/30:\nTrain Loss: 41.3348\nVal RMSE: 13.1444, Val R2: 0.3554\nEpoch 25/30:\nTrain Loss: 34.6764\nVal RMSE: 13.2134, Val R2: 0.3486\nEpoch 26/30:\nTrain Loss: 32.3003\nVal RMSE: 13.2720, Val R2: 0.3428\nEpoch 27/30:\nTrain Loss: 30.8254\nVal RMSE: 13.2233, Val R2: 0.3476\nEpoch 28/30:\nTrain Loss: 29.4683\nVal RMSE: 13.3030, Val R2: 0.3397\nEpoch 29/30:\nTrain Loss: 28.3595\nVal RMSE: 13.4438, Val R2: 0.3257\nEpoch 30/30:\nTrain Loss: 26.7281\nVal RMSE: 13.3875, Val R2: 0.3313\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    reduced_train_text = model.text_encoder(train_dataset.text_features.to(device)).cpu().numpy()\n    reduced_test_text = model.text_encoder(test_dataset.text_features.to(device)).cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:32:45.652072Z","iopub.execute_input":"2024-12-30T06:32:45.652439Z","iopub.status.idle":"2024-12-30T06:32:46.770942Z","shell.execute_reply.started":"2024-12-30T06:32:45.652404Z","shell.execute_reply":"2024-12-30T06:32:46.770250Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:32:46.771680Z","iopub.execute_input":"2024-12-30T06:32:46.771889Z","iopub.status.idle":"2024-12-30T06:32:46.777342Z","shell.execute_reply.started":"2024-12-30T06:32:46.771872Z","shell.execute_reply":"2024-12-30T06:32:46.776535Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"EnrollmentModel(\n  (text_encoder): TextEncoder(\n    (encoder): Sequential(\n      (0): Linear(in_features=10180, out_features=1024, bias=True)\n      (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (2): ReLU()\n      (3): Dropout(p=0.2, inplace=False)\n      (4): Linear(in_features=1024, out_features=512, bias=True)\n      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (6): ReLU()\n      (7): Dropout(p=0.2, inplace=False)\n      (8): Linear(in_features=512, out_features=256, bias=True)\n    )\n  )\n  (text_decoder): TextDecoder(\n    (decoder): Sequential(\n      (0): Linear(in_features=256, out_features=512, bias=True)\n      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (2): ReLU()\n      (3): Dropout(p=0.2, inplace=False)\n      (4): Linear(in_features=512, out_features=1024, bias=True)\n      (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (6): ReLU()\n      (7): Dropout(p=0.2, inplace=False)\n      (8): Linear(in_features=1024, out_features=10180, bias=True)\n    )\n  )\n  (predictor): EnrollmentPredictor(\n    (predictor): Sequential(\n      (0): Linear(in_features=304, out_features=256, bias=True)\n      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (2): ReLU()\n      (3): Dropout(p=0.2, inplace=False)\n      (4): Linear(in_features=256, out_features=128, bias=True)\n      (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (6): ReLU()\n      (7): Dropout(p=0.2, inplace=False)\n      (8): Linear(in_features=128, out_features=1, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"from sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error, adjusted_rand_score\nimport joblib\nimport cupy as cp  # For GPU support\nimport pandas as pd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:32:46.778243Z","iopub.execute_input":"2024-12-30T06:32:46.778550Z","iopub.status.idle":"2024-12-30T06:32:47.892068Z","shell.execute_reply.started":"2024-12-30T06:32:46.778520Z","shell.execute_reply":"2024-12-30T06:32:47.891176Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print(reduced_train_text.shape, X_train_num.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:35:33.264081Z","iopub.execute_input":"2024-12-30T06:35:33.264482Z","iopub.status.idle":"2024-12-30T06:35:33.269469Z","shell.execute_reply.started":"2024-12-30T06:35:33.264442Z","shell.execute_reply":"2024-12-30T06:35:33.268423Z"}},"outputs":[{"name":"stdout","text":"(55168, 256) (55168, 48)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def adjusted_r2_score(r2, n_samples, n_features):\n    \"\"\"Calculate adjusted R2 score\"\"\"\n    return 1 - (1 - r2) * (n_samples - 1) / (n_samples - n_features - 1)\n\nX_trian_combined = None\nX_test_combined = None\n\nX_train_combined = hstack([csr_matrix(reduced_train_text), csr_matrix(X_train_num)])\nX_test_combined = hstack([csr_matrix(reduced_test_text), csr_matrix(X_test_num)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:39:57.871529Z","iopub.execute_input":"2024-12-30T06:39:57.871868Z","iopub.status.idle":"2024-12-30T06:39:58.718119Z","shell.execute_reply.started":"2024-12-30T06:39:57.871841Z","shell.execute_reply":"2024-12-30T06:39:58.717153Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(X_train_combined.shape, X_test_combined.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T06:40:25.025978Z","iopub.execute_input":"2024-12-30T06:40:25.026346Z","iopub.status.idle":"2024-12-30T06:40:25.030861Z","shell.execute_reply.started":"2024-12-30T06:40:25.026287Z","shell.execute_reply":"2024-12-30T06:40:25.030034Z"}},"outputs":[{"name":"stdout","text":"(55168, 304) (13792, 304)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"X_train_combined = X_train_combined.astype('float32')\nX_test_combined = X_test_combined.astype('float32')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T07:16:09.811796Z","iopub.execute_input":"2024-12-30T07:16:09.812126Z","iopub.status.idle":"2024-12-30T07:16:09.877162Z","shell.execute_reply.started":"2024-12-30T07:16:09.812100Z","shell.execute_reply":"2024-12-30T07:16:09.876246Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# import pickle\n\n# # Save the arrays using pickle\n# with open(\"X_train_combined.pkl\", \"wb\") as f:\n#     pickle.dump(X_train_combined, f)\n\n# with open(\"X_test_combined.pkl\", \"wb\") as f:\n#     pickle.dump(X_test_combined, f)\n\n# with open(\"y_train.pkl\", \"wb\") as f:\n#     pickle.dump(y_train, f)\n\n# with open(\"y_test.pkl\", \"wb\") as f:\n#     pickle.dump(y_test, f)\n\n# print(\"Data saved using pickle.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T07:24:14.193602Z","iopub.execute_input":"2024-12-30T07:24:14.193935Z","iopub.status.idle":"2024-12-30T07:24:14.477115Z","shell.execute_reply.started":"2024-12-30T07:24:14.193912Z","shell.execute_reply":"2024-12-30T07:24:14.476397Z"}},"outputs":[{"name":"stdout","text":"Data saved using pickle.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error, adjusted_rand_score\nimport joblib\nimport pandas as pd\nimport numpy as np\n\ndef adjusted_r2_score(r2, n_samples, n_features):\n    \"\"\"Calculate adjusted R2 score\"\"\"\n    return 1 - (1 - r2) * (n_samples - 1) / (n_samples - n_features - 1)\n\ndef compute_scores(model, X_train, X_test, y_train, y_test, feature_set, use_gpu=True):\n    if use_gpu and isinstance(model, XGBRegressor):\n        model.set_params(tree_method='gpu_hist')  # Enable GPU acceleration for XGBoost\n    \n    # Convert to dense array if sparse\n    X_train = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n    X_test = X_test.toarray() if hasattr(X_test, 'toarray') else X_test\n    \n    # Fit the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    # Calculate metrics\n    train_r2 = r2_score(y_train, y_train_pred)\n    test_r2 = r2_score(y_test, y_test_pred)\n    \n    # Calculate adjusted R2\n    n_train_samples, n_features = X_train.shape\n    n_test_samples = X_test.shape[0]\n    \n    train_adj_r2 = adjusted_r2_score(train_r2, n_train_samples, n_features)\n    test_adj_r2 = adjusted_r2_score(test_r2, n_test_samples, n_features)\n\n    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n\n    return {\n        \"Feature Set\": feature_set,\n        \"Train R2\": train_r2,\n        \"Test R2\": test_r2,\n        \"Train Adj R2\": train_adj_r2,\n        \"Test Adj R2\": test_adj_r2,\n        \"Train RMSE\": train_rmse,\n        \"Test RMSE\": test_rmse\n    }\n\n# Model parameters\n\nxgb_params = {\n    'n_estimators': 1000,\n    'learning_rate': 0.01,\n    'max_depth': 6,\n    'min_child_weight': 1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'random_state': 42,\n    'n_jobs': -1  # Use all CPU cores\n}\n\nxgb_model = XGBRegressor(**xgb_params)\n\n# Placeholder for results\nresults = []\n\n# 3. Combined Features\nprint(\"Training models with combined features...\")\n\nresults.append(compute_scores(xgb_model, X_train_combined, X_test_combined, \n                            y_train, y_test, \"Combined\"))\n\n# Convert results to a DataFrame\nresults_df = pd.DataFrame(results, \n                         index=[\"XGB - Combined\"])\n\n# Save the results\nresults_df.to_csv('model_comparison_results.csv')\nprint(\"\\nModel Comparison Results:\")\nprint(results_df)\n\n# Save the best model (assuming XGBoost with combined features performs best)\nbest_model = xgb_model\njoblib.dump(best_model, 'best_model_ml.joblib')\n\n# Feature importance for XGBoost (for combined features)\nif isinstance(best_model, XGBRegressor):\n    feature_importance = pd.DataFrame({\n        'feature': range(X_train_combined.shape[1]),\n        'importance': best_model.feature_importances_\n    })\n    feature_importance = feature_importance.sort_values('importance', ascending=False)\n    print(\"\\nTop 10 Most Important Features:\")\n    print(feature_importance.head(20))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}